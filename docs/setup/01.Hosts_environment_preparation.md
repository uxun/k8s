# Hosts environment preparation

## 1.Installation

$ ansible-playbook [01.prepare.yml](../../01.prepare.yml)

```shell
# Role of application
roles/deploy	# Key certificate configuration
roles/prepare	# Environment configuration，Distribute keys and certificates
roles/lb	# Configure the HA 
roles/chrony	# Synchronous time configuration
```

## 2.Configuration

### 2.1 Roles:deploy

```shell
roles/deploy/
├── tasks
│   └── main.yml
└── templates
    ├── admin-csr.json.j2	# kubectl客户端使用的证书请求模板
    ├── ca-config.json.j2	# ca 配置文件模板
    ├── ca-csr.json.j2	# ca 证书签名请求模板
    ├── read-csr.json.j2	# 只读
    └── kube-proxy-csr.json.j2	# kube-proxy使用的证书请求模板
```

kubernetes 系统各组件需要使用 TLS 证书对通信进行加密。 [参考](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/)

CloudFlare 的 PKI 工具集生成自签名的 CA 证书，用来签名后续创建的其它 TLS 证书。[参考阅读](https://coreos.com/os/docs/latest/generate-self-signed-certificates.html)

根据认证对象可以将证书分成三类：

| Name        | Info                                   |
| ----------- | -------------------------------------- |
| server cert | 服务器证书                             |
| client cert | 客户端证书                             |
| peer cert   | 可以是server cert，也可以是client cert |

在kubernetes 集群中需要的证书种类如下：[参考](https://kubernetes.io/docs/setup/certificates/#single-root-ca)

| Node                        | Certificate                |                                                              |
| --------------------------- | -------------------------- | ------------------------------------------------------------ |
| etcd                        | server cert \| client cert | 标识自己服务的`server cert`，也需要`client cert`与`etcd`集群其他节点交互，当然可以分别指定2个证书，也可以使用一个对等证书 |
| master                      | server cert \| client cert | 标识 apiserver服务的`server cert`，也需要`client cert`连接`etcd`集群，这里也使用一个对等证书 |
| kubectl，calico，kube-proxy | client cert                | 只需要`client cert`，因此证书请求中 `hosts` 字段可以为空     |
| kubelet                     |                            | 证书比较特殊，不是手动生成，它由node节点`TLS BootStrap` 向`apiserver`请求，由`master`节点的`controller-manager` 自动签发，包含一个`client cert` 和一个`server cert` |

整个集群要使用统一的CA 证书，只需要在 deploy 节点创建，然后分发给其他节点；为了保证安装的幂等性，如果已经存在CA 证书，就跳过创建CA 步骤

#### 2.2 Create CA configuration  [ca-config.json.j2](../../roles/deploy/templates/ca-config.json.j2)

```yaml
{
  "signing": {	#表示该证书可用于签名其它证书；生成的 ca.pem 证书中 `CA=TRUE`；
    "default": {
      "expiry": "87600h"
    },
    "profiles": {	#包含了`server auth`和`client auth`，所以可以签发三种不同类型证书；
      "kubernetes": {
        "usages": [
            "signing",
            "key encipherment",
            "server auth",	#表示可以用该 CA 对 server 提供的证书进行验证；
            "client auth"	#表示可以用该 CA 对 client 提供的证书进行验证；
        ],
        "expiry": "87600h"
      }
    }
  }
}
```

#### 2.3 Create CA Certificate signature request  [ca-csr.json.j2](../../roles/deploy/templates/ca-csr.json.j2)

```yaml
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Shenzhen",
      "L": "GS",
      "O": "k8s",
      "OU": "System"
    }
  ],
  "ca": {
    "expiry": "131400h"
  }
}
```

#### 2.4 Generate the CA certificate and private key

`cfssl gencert -initca ca-csr.json | cfssljson -bare ca`

#### 2.5 Generate the kubeconfig configuration file

kubectl 使用 ~/.kube/config 配置文件与kube-apiserver进行交互，且拥有管理 K8S集群的完全权限，

准备kubectl使用的admin 证书签名请求 [admin-csr.json.j2](../../roles/deploy/templates/admin-csr.json.j2)

``` yaml
{
  "CN": "admin",
  "hosts": [],	# kubectl 使用客户端证书可以不指定hosts 字段
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Shenzhen",
      "L": "GS",
      "O": "system:masters",	# RBAC 预定义的 ClusterRoleBinding 将 Group system:masters 与 ClusterRole cluster-admin 绑定，这就赋予了kubectl所有集群权限
      "OU": "System"
    }
  ]
}
```
###### example："O": "system:masters"

``` bash
$ kubectl describe clusterrolebinding cluster-admin
Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind   Name            Namespace
  ----   ----            ---------
  Group  system:masters  
```

#### 2.6 Generate the cluster-admin user certificate

```shell
$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
```

#### 2.7 Generate ~/. Kube /config configuration file

使用`kubectl config` 生成kubeconfig 自动保存到 ~/.kube/config，生成后 `cat ~/.kube/config`可以验证配置文件包含 kube-apiserver 地址、证书、用户名等信息。

```shell
# 设置集群参数
$ kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=127.0.0.1:8443
# 设置客户端认证参数
$ kubectl config set-credentials admin --client-certificate=admin.pem --embed-certs=true --client-key=admin-key.pem
# 设置上下文参数
$ kubectl config set-context kubernetes --cluster=kubernetes --user=admin
# 选择默认上下文
$ kubectl config use-context kubernetes
```

#### 2.8 Generate the kube-proxy.kubeconfig configuration file

Create the kube-proxy certificate request

``` yaml
{
  "CN": "system:kube-proxy",	# CN User为system:kube-proxy，预定义的 ClusterRoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，授予了调用 kube-apiserver Proxy 相关 API 的权限；
  "hosts": [],	# 使用客户端证书可以不指定hosts 字段
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "HangZhou",
      "L": "XS",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
```
###### example："CN": "system:kube-proxy"

``` bash
$ kubectl describe clusterrolebinding system:node-proxier
Name:         system:node-proxier
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate=true
Role:
  Kind:  ClusterRole
  Name:  system:node-proxier
Subjects:
  Kind  Name               Namespace
  ----  ----               ---------
  User  system:kube-proxy  
```

#### 2.9 Generate the kube-proxy user certificate

```shell
$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
```

#### 2.10 Generate kube - proxy. Kubeconfig

使用`kubectl config` 生成kubeconfig 自动保存到 kube-proxy.kubeconfig

```shell
$ kubectl config set-cluster kubernetes --certificate-authority=ca.pem --embed-certs=true --server=127.0.0.1:8443 --kubeconfig=kube-proxy.kubeconfig
$ kubectl config set-credentials kube-proxy --client-certificate=kube-proxy.pem --embed-certs=true --client-key=kube-proxy-key.pem --kubeconfig=kube-proxy.kubeconfig
$ kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig
$ kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

### 3.Roles:prepare

------

⌘ + [roles/prepare/tasks/main.yml](../../roles/prepare/tasks/main.yml) 请阅读脚本中的注释内容

``` bash
roles/prepare
├── tasks
│   ├── centos.yml	# disable {selinux,firewalld,selinux},yum epel
│   ├── common.yml	# 系统参数{swap,kernel module,ulimits},k8s-sysctl.conf 
│   ├── debian.yml	
│   └── main.yml	# create DIR,cfssl,easzctl,$PATH
└── templates
    ├── 10-k8s-modules.conf.j2
    ├── 30-k8s-ulimits.conf.j2
    └── 95-k8s-sysctl.conf.j2
```
### 4.Roles:lb

------

⌘ + [roles/lb/tasks/main.yml](../../roles/lb/tasks/main.yml) Task

``` bash
roles/lb
├── defaults
│   └── main.yml
├── lb.yml
├── tasks
│   └── main.yml
└── templates
    ├── haproxy.cfg.j2
    ├── haproxy.service.j2
    ├── keepalived-backup.conf.j2
    └── keepalived-master.conf.j2
```

| Haproxy                                                      | Keepalived                                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 支持四层和七层负载，稳定性好                                 | 它是基于VRRP协议保证所谓的高可用或热备的，这里用来预防haproxy的单点故障。 |
| HAProxy可以跑满10Gbps-New benchmark of HAProxy at 10 Gbps using Myricom's 10GbE NICs (Myri-10G PCI-Express) | keepalived利用vrrp协议生成一个虚拟地址(VIP)，正常情况下VIP存活在keepalive的主节点，当主节点故障时，VIP能够漂移到keepalived的备节点，保障VIP地址可用性 |
| haproxy的配置中配置多个后端真实kube-apiserver的endpoints，并启用存活监测后端kube-apiserver，如果一个kube-apiserver故障，haproxy会将其剔除负载池 | keepalived利用vrrp协议生成一个虚拟地址(VIP)，正常情况下VIP存活在keepalive的主节点，当主节点故障时，VIP能够漂移到keepalived的备节点，保障VIP地址可用性 |

#### 4.1 Install haproxy

`$ yum install haproxy`

##### Configuration haproxy ⌘+ [haproxy.cfg.j2](../../roles/lb/templates/haproxy.cfg.j2)

``` shell
global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /run/haproxy/admin.sock mode 660 level admin
        stats timeout 30s
        user haproxy
        group haproxy
        daemon
        nbproc 1

defaults
        log     global
        timeout connect 5000
        timeout client  50000
        timeout server  50000

listen kube-master  # name kube-master
        bind 0.0.0.0:{{ KUBE_APISERVER.split(':')[2] }} #监听客户端请求的地址/端口，保证监听master的VIP地址和端口
        mode tcp	#选择四层负载模式 (当然你也可以选择七层负载，请查阅指南，适当调整)
        option tcplog
        balance source	#选择负载算法 (负载算法也有很多供选择)
        server s1 {{ master1 }}  check inter 10000 fall 2 rise 2 weight 1	#配置master节点真实的endpoits，必须与hosts文件对应设置
        server s2 {{ master2 }}  check inter 10000 fall 2 rise 2 weight 1
```
#### 4.2 Install keepalived

`$ yum install keepalived`

##### 配置keepalived主节点 ⌘+ [keepalived-master.conf.j2](../../roles/lb/templates/keepalived-master.conf.j2)

``` shell
global_defs {
    router_id lb-master
}

vrrp_script check-haproxy { # 定义了监测haproxy进程的脚本，利用shell 脚本killall -0 haproxy 进行检测进程是否存活，如果进程不存在，根据weight -30设置将主节点优先级降低30，这样原先备节点将变成主节点。
    script "killall -0 haproxy"
    interval 5
    weight -30
}

vrrp_instance VI-kube-master { # 定义了vrrp组，包括优先级、使用端口、router_id、心跳频率、检测脚本、虚拟地址VIP等
    state MASTER
    priority 120
    dont_track_primary
    interface {{ LB_IF }}
    virtual_router_id {{ ROUTER_ID }} # 标识了一个 VRRP组，在同网段下必须唯一，否则出现 Keepalived_vrrp: bogus VRRP packet received on eth0 !!!类似报错
    advert_int 3
    track_script {
        check-haproxy
    }
    virtual_ipaddress {
        {{ MASTER_IP }}
    }
}
```
##### 配置keepalived备节点 ⌘+ [keepalived-backup.conf.j2](../../roles/lb/templates/keepalived-backup.conf.j2)
``` shell
global_defs {
    router_id lb-backup
}

vrrp_instance VI-kube-master {
    state BACKUP
    priority 110 # 备节点权重小于主节点
    dont_track_primary
    interface {{ LB_IF }}
    virtual_router_id {{ ROUTER_ID }} # 与主节点一致
    advert_int 3 # 与主节点一致
    virtual_ipaddress { 
        {{ MASTER_IP }} # 与主节点一致
    }
}
```
#### 4.3 Verification keepalived and haproxy 

+ lb 节点验证

``` bash
$ systemctl status haproxy 	# 检查进程状态
$ journalctl -u haproxy		# 检查进程日志是否有报错信息

$ systemctl status keepalived 	# 检查进程状态
$ journalctl -u keepalived	# 检查进程日志是否有报错信息

$ netstat -antlp|grep 8443	# 检查tcp端口是否监听
```
+ 在 keepalived 主节点

``` bash
$ ip a				# 检查 master的 VIP地址是否存在
```
