# 05.Install_Kube-node

> 安装kubelet，kube-proxy，cni-plugins，haproxy [可选]
>
> 节点：kube-node

### OVERVIEW

##### `$ ansible-playbook` [05.kube-node.yml](../../05.kube-node.yml)

1.Dir and Dowload some binary {kubelet,kube-proxy,[cni-plugins](https://github.com/containernetworking/plugins/releases/download/v0.7.5/cni-plugins-amd64-v0.7.5.tgz)} to moving

2.In the node deployment haroxy [用于node节点转发多个apiserver]

3.CNI 配置

4.配置kubelet {CA，kubelet.kubeconfig，service}

5.配置kube-proxy 

------

### 1.Certificate of configuration 

##### 证书签名请求 [kublet-csr.json.j2](../../roles/kube-node/templates/kubelet-csr.json.j2)

##### kube-node 部署步骤 [roles/kube-node/tasks/main.yml](../../roles/kube-node/tasks/main.yml) 

### 2.Deployment HA

> Node 节点运行 haproxy 连接到多个 apiserver, kube-node节点不能同时为ex-lb成员
>
> - Master节点的 kube-controller-manager、kube-scheduler 是多实例部署，所以只要有一个实例正常，就可以保证高可用；
> - Cluster的 Pod 使用 K8S 服务域名 kubernetes 访问 kube-apiserver， kube-dns 会自动解析出多个 kube-apiserver 节点的 IP，所以也是高可用的；
> - 在每个节点起个haproxy进程，后端对接多个 apiserver 实例，对它们做健康检查和负载均衡；
> - kubelet、kube-proxy、controller-manager、scheduler 通过本地的 haproxy（监听 127.0.0.1）访问 kube-apiserver，从而实现 kube-apiserver 的高可用；

### 3.Create CNI config

> `DaemonSet Pod`方式运行k8s网络插件，所以kubelet.server服务必须开启cni相关参数，并且提供cni网络配置文件

### 4.kubelet service

先创建工作目录 `/var/lib/kubelet`  [Options](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/#options)

```
需要注意的配置
--anonymous-auth=false	# 关闭kubelet的匿名访问
--cluster-dns=	#指定 kubedns 的 Service IP(可以先分配，后续创建 kubedns 服务时指定该 IP)，--cluster-domain 指定域名后缀，这两个参数同时指定后才会生效；
--fail-swap-on=false	# K8S 1.8+需显示禁用这个，否则服务不能启动
--network-plugin=cni	# 为使用cni 网络，并调用calico管理网络所需的配置
```

### 5 kube-proxy kubeconfig 

该步骤已经在 deploy节点完成，[roles/deploy/tasks/main.yml](../../roles/deploy/tasks/main.yml)

+ 生成的kube-proxy.kubeconfig 配置文件需要移动到/etc/kubernetes/目录，后续kube-proxy服务启动参数里面需要指定

### kube-proxy service [Options](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/#options)

> 特别注意：
>
> kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；但是这个特性与calico 实现 network policy冲突，所以如果要用 network policy，这两个选项都不要指定。

``` bash
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart={{ bin_dir }}/kube-proxy \
  --bind-address={{ inventory_hostname }} \
  --hostname-override={{ inventory_hostname }} \ #参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 iptables 规则
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
  --logtostderr=true \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

### Validation

``` bash
# 查看状态
systemctl status kubelet
systemctl status kube-proxy
# 查看日志
journalctl -u kubelet	
journalctl -u kube-proxy 
```
##### kubectl cordon {{ inventory_hostname }} 04.kube-master.yml 

将节点标记不可调度pod

``` bash
$ kubectl get node
NAME           STATUS                     ROLES    AGE    VERSION
192.168.0.25   Ready,SchedulingDisabled   master   5d7h   v1.13.3
192.168.0.26   Ready,SchedulingDisabled   master   5d7h   v1.13.3
192.168.0.27   Ready                      node     5d7h   v1.13.3
192.168.0.29   Ready                      node     5d7h   v1.13.3
```
